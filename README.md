# Parth_GenAIStack

#Video Demo: https://drive.google.com/file/d/1UpMmusUc3UlxwwDe1QKHkpGzlaW35s11/view

This repository contains the code and resources for the GenAI Stack project, which aims to explore and develop applications using various generative AI models and frameworks.

## Overview

The GenAI Stack project focuses on leveraging the power of generative AI models, such as large language models (LLMs) and other AI technologies, to build innovative applications and solutions. The repository serves as a central hub for experimenting with different AI models, integrating them with various frameworks, and developing proof-of-concept applications.

## Features

- **AI Model Integration**: The repository provides examples and code snippets for integrating popular AI models, such as GPT-3, Llama, Phi-3, and others, into your applications.
- **Framework Exploration**: It explores the integration of AI models with various frameworks and libraries, such as React (Vite), FastAPI, OpenAGI, and more, enabling the development of web applications, APIs, and other software solutions.
- **Proof-of-Concept Applications**: The repository includes proof-of-concept applications that showcase the capabilities of generative AI models in different domains, such as natural language processing, text generation, and data analysis.
- **Documentation and Tutorials**: Detailed documentation and tutorials are provided to help developers understand the concepts, setup instructions, and best practices for working with AI models and frameworks.

## Getting Started

To get started with the GenAI Stack project, follow these steps:

1. Clone the repository: `git clone https://github.com/Parthos5/Parth_GenAIStack.git`
2. Navigate to the `client` folder: `cd client`
3. Install the client dependencies: `npm install`
4. Start the client development server: `npm run dev`
5. In a separate terminal, navigate to the `server` folder: `cd server`
6. Activate the virtual environment: `venv/Scripts/activate` (Windows) or `source venv/bin/activate` (Unix/macOS)
7. Start the server: `python main.py`
8. Access the server documentation at `http://127.0.0.1:8000/docs`

The client application will be running on `http://localhost:5173`, and the server will be running on `http://127.0.0.1:8000`.

## Server Documentation

The server is built using FastAPI, a modern, fast (high-performance), web framework for building APIs with Python. The server documentation is available at `http://127.0.0.1:8000/docs` and provides detailed information about the available endpoints, request/response formats, and examples.
